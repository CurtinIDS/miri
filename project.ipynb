{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transient object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to develop a workflow for generating synthetic data and training a machine learning model to detect meteors from images.\n",
    "\n",
    "This tutorial has been prepared by:\n",
    "- [Andrew Rohl](http://computation.curtin.edu.au/about/steering-committee/director/)\n",
    "- [Shiv Meka](http://computation.curtin.edu.au/about/computational-specialists/humanities/)\n",
    "- [Kevin Chai](http://computation.curtin.edu.au/about/computational-specialists/health-sciences/)\n",
    "\n",
    "from the [Curtin Institute for Computation](http://computation.curtin.edu.au) at Curtin University in Perth, Australia for the [7th International Conference on Smart Computing & Communications (ICSCC 2019)](http://icscc.online/) hosted at Curtin University in Miri, Sarawak, Malaysia on the 28-30 June 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Curtin Institute for Computation was asked to develop a machine learning model to detect meteor trails (fireballs) by researchers from the [Curtin Institute for Radio Astronomy](http://astronomy.curtin.edu.au/) ([ICRAR](https://www.icrar.org/) - Curtin University node) from images of the night sky captured in the Australian desert by the [Desert Fireball Network](http://fireballsinthesky.com.au/). \n",
    "\n",
    "![Fireballs](assets/desert_fireball_network.png)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Desert Fireball Network camera locations</div>\n",
    "\n",
    "The objective of the project is to find these meteors in optical images and to compare against the radio emissions recorded at the same time and location by the [Murchison Widefield Array](http://www.mwatelescope.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Curtin Institute for Radio Astronomy (CIRA)*\n",
    "* Paul Hancock\n",
    "* Xiang Zhang\n",
    "* Sean Mattingley\n",
    "* Steven Tingay\n",
    "* Randall Wayth\n",
    "\n",
    "*Desert Fireball Network*\n",
    "* Hadrien Devillepoix\n",
    "* Phil Bland\n",
    "\n",
    "*Curtin Institute for Computation*\n",
    "* Shiv Meka\n",
    "* Kevin Chai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is comprised of images taken during the Geminid meteor shower on the 14th of December 2015. Images were captured from a camera based at a station and a camera that was set up on the back of a truck (mobile) to capture images from multiple locations. The images were reviewed and annotated by astronomers to construct the dataset shown in Table 1.\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 1: Dataset</p>\n",
    "\n",
    "| Camera      | Images    | Meteors\n",
    "|:------------|:----------|:--------|\n",
    "| mobile      | 1,561     | 48      |\n",
    "| station     | 1,330     | 24      |\n",
    "\n",
    "The raw dataset contains RGB images with a resolution of 7360x4912 pixels (~6.3MB each). Meteors within these images were manually identified by astronomers within the project team. Examples of these meteors are shown below.\n",
    "\n",
    "![Fireballs](assets/meteors.png)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Cropped images containing meteors in the dataset</div>\n",
    "\n",
    "From preliminary experiments, we observed that meteors could be identified without colour and using lower resolution images. The benefit of compressing the images is that it reduces the computational overhead required and therefore allows us to train a meteor detection model faster. \n",
    "\n",
    "The raw images are transformed into grayscale and to a resolution of 1840x1228, resulting in ~240KB for each image. An example is shown below.\n",
    "\n",
    "![Night Sky](assets/night_sky_example.jpg)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Transformed night sky image example</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data preparation pipeline can performed using the Python Imaging Library (PIL / Pillow) module. An example is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from PIL import Image, ImageDraw\n",
    "from workshop import *\n",
    "\n",
    "img = Image.open('assets/data_prep_example.jpg')\n",
    "\n",
    "# Resize the new image to 50% of the original\n",
    "img_new = img.resize((int(img.width * 0.5), int(img.height * 0.5)), Image.ANTIALIAS)\n",
    "\n",
    "# Convert to grayscale\n",
    "img_new = np.array(img_new.convert('L'))\n",
    "\n",
    "# Plot the original and transformed image\n",
    "# Original\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "fig.gca().title.set_text('Original')\n",
    "fig.gca().imshow(img);\n",
    "\n",
    "# resized, grayscale\n",
    "fig = plt.figure(figsize=(3.5, 3.5))\n",
    "fig.gca().title.set_text('50% resized, grayscale')\n",
    "fig.gca().imshow(img_new, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not contain enough meteors to train a robust and accurate machine learning model. The research team simplified the problem to identify any transient objects (streaks) in the images. Transient objects can be meteors, satellites and planes.\n",
    "\n",
    "Therefore, we experimented with generating a synthetic dataset of transient objects to train our model. We manually identified images of the night sky that contained no streaks and used these as the background to generate our synthetic / fake meteor images.\n",
    "\n",
    "Let's inspect two background examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and transformed image\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 16))\n",
    "img1 = Image.open('assets/background1.jpg')\n",
    "ax1.imshow(np.asarray(img1), cmap='gray');\n",
    "ax1.set_title('Background images')\n",
    "img2 = Image.open('assets/background2.jpg')\n",
    "ax2.imshow(np.asarray(img2), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the second image is more illuminated than the first with less visible stars. This is because the second image was taken closer to dawn / sunrise. We use 272 different background images to ensure enough variation (different real world conditions) is added to our dataset.\n",
    "\n",
    "Define functions to create tiled images (200x200 pixel resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes distance between two points in pixels\n",
    "def dist(A): \n",
    "    return(((A[0][0] - A[0][2])**2 + (A[0][1] - A[0][3])**2)**0.5)\n",
    "\n",
    "def generate_streaks(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    tile_w = 200 \n",
    "    tile_h = 200\n",
    "    n=10\n",
    "    images = []\n",
    "\n",
    "    for __ in range(n):\n",
    "        bg_rand = np.random.choice([1, 2])    \n",
    "        img = Image.open('assets/background' + str(bg_rand) + '.jpg')\n",
    "        width, height = img.size\n",
    "        marker_x = np.random.randint(0, width - tile_w)\n",
    "        marker_y = np.random.randint(0, height - tile_h)\n",
    "\n",
    "        # crop a portion of the full image\n",
    "        im = img.crop((marker_x, marker_y, marker_x + tile_w, marker_y + tile_h))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "\n",
    "        A = [(np.random.rand() * im.size[0], np.random.rand() * im.size[1],\n",
    "              np.random.rand() * im.size[0], np.random.rand() * im.size[1])]\n",
    "\n",
    "        # The lines shouldn't be too short or too long\n",
    "        while dist(A) <= 30 or dist(A) >= 300:\n",
    "            A = [(np.random.rand() * im.size[0], np.random.rand() * im.size[1],\n",
    "                  np.random.rand() * im.size[0], np.random.rand() * im.size[1])]\n",
    "\n",
    "        if dist(A) > 30 and dist(A) < 300: \n",
    "            # Brightness should be over certain threshold, \n",
    "            # lower the brightness -> harder to train, more resilient\n",
    "            color_rnd = np.random.uniform(0.8, 1.0)  \n",
    "            width_rand = np.random.choice([1, 2])\n",
    "            draw.line([A[0][0], A[0][1], A[0][2], A[0][3]], fill=int(color_rnd * 250), width=width_rand)\n",
    "            del draw\n",
    "        \n",
    "        images.append(im)\n",
    "        \n",
    "    plot_streaks(images)\n",
    "\n",
    "def plot_streaks(images):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    columns = 5\n",
    "    for i, image in enumerate(images):\n",
    "        ax = plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "        ax.imshow(np.asarray(image), cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interact(generate_streaks, \n",
    "         seed=widgets.IntSlider(min=0, max=10, step=1, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently our function only generates image tiles with streaks. Now let's define a more complete function that will generate image with/without streaks and assign the correct label (0 = no streak, 1 = streak) to the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_image(file_name=\"meteors.hd5\",batch_size=80,seed=3):\n",
    "    '''\n",
    "    file_name: hdf file with images of night_sky and validation\n",
    "    batch_size: images per step\n",
    "    '''\n",
    "    filter_w=200 #output image width\n",
    "    filter_h=200 #output image height\n",
    "    np.random.seed(seed)\n",
    "    def dist(A): #Computes distance between two points in px\n",
    "        return(((A[0][0]-A[0][2])**2+(A[0][1]-A[0][3])**2)**0.5)\n",
    "\n",
    "    count=0\n",
    "    hdf=h5py.File(\"meteors.hd5\",\"r\")\n",
    "    bg_imgs=hdf[\"night_sky\"][:]\n",
    "    while True:\n",
    "        out_images=[] #list to store\n",
    "        out_preds=[]\n",
    "        for i in range(batch_size):\n",
    "            myrand=np.random.randint(bg_imgs.shape[0]) #Open files with random background and w/o meteorites\n",
    "            im=Image.fromarray(bg_imgs[myrand]) #im stores images with random background\n",
    "            width,height=im.size\n",
    "            marker_x=np.random.randint(0,width-filter_w)\n",
    "            marker_y=np.random.randint(0,width-filter_h)\n",
    "            #crop a portion of the image\n",
    "            im=im.crop((marker_x,marker_y,marker_x+filter_w,marker_y+filter_h))\n",
    "            draw = ImageDraw.Draw(im)\n",
    "            A=[(np.random.rand()*im.size[0], np.random.rand()*im.size[1],\n",
    "                np.random.rand()*im.size[0], np.random.rand()*im.size[1])]\n",
    "            myrand=str(myrand)\n",
    "            flag_present=False\n",
    "            if (np.random.rand()>0.5): #Statistically, only half the samples would have meteorites\n",
    "                if ((dist(A)>50) and (dist(A)<400)): #The lines shouldn't be too short or too long\n",
    "                    color_rnd=np.random.uniform(0.5,0.8)  #Brightness should be over certain threshold, lower the brightness -> harder to train, more resilient\n",
    "                    width_rand=np.random.choice([1,2])\n",
    "                    draw.line([A[0][0],A[0][1],A[0][2],A[0][3]], fill=int(color_rnd*100),width=width_rand)\n",
    "                    flag_present=True\n",
    "                    del draw\n",
    "            out_images.append(np.array(im).reshape([filter_w,filter_h,1]))\n",
    "            out_preds.append(flag_present*1)\n",
    "        yield np.array(out_images),np.array(out_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize images\n",
    "out = next(synthetic_images(batch_size=1))\n",
    "print('Label: %d' % (out[1][0]))\n",
    "Image.fromarray(out[0][0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through our experimentation, we designed a CNN model architecture that was able to achieve good classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def create_model(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Input image tiles are 200 x 200 x 1 images\n",
    "    inp = Input(shape=(200, 200, 1))\n",
    "\n",
    "#     # 3 sets of convolution + pooling layers, REctified Linear Unit activation (non linear)\n",
    "#     x = Conv2D(8, 3, activation='relu')(inp)\n",
    "#     x = MaxPool2D()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv2D(5, 3, activation='relu')(x)\n",
    "#     x = MaxPool2D()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv2D(4, 3, activation='relu')(x)\n",
    "#     x = MaxPool2D()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "    # 5 fully connected layers\n",
    "    for __ in range(3):\n",
    "        x = Dense(5)(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer='adagrad', loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 1\n",
    "model = create_model(seed)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our CNN model using the `fit_generator` method. This makes use of our synthetic_images function that generates our streaks and no streaks images on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('meteors_validation/*/*jpg'))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for index, file in enumerate(files):\n",
    "    img = Image.open(file)\n",
    "    data.append(np.array(img))\n",
    "    if index < 4:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "        \n",
    "labels = np.array(labels)\n",
    "data = np.array(data).reshape(len(labels), 200, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 9s 315ms/step - loss: 0.7024 - val_loss: 0.7038\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 10s 319ms/step - loss: 0.6345 - val_loss: 0.7098\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 10s 329ms/step - loss: 0.5293 - val_loss: 0.6063\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 14s 453ms/step - loss: 0.5124 - val_loss: 0.4998\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 11s 356ms/step - loss: 0.4434 - val_loss: 0.5540\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 13s 426ms/step - loss: 0.3919 - val_loss: 0.4294\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 14s 471ms/step - loss: 0.3149 - val_loss: 0.4230\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 11s 383ms/step - loss: 0.3091 - val_loss: 0.4250\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 12s 402ms/step - loss: 0.2003 - val_loss: 0.1880\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 11s 372ms/step - loss: 0.1776 - val_loss: 0.1925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ffb0cb320>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~2 mins to train \n",
    "#model = create_model(2)\n",
    "seed=100\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "input_layer=Input(shape=(200,200,1)) #1 - channel implies just grayscale\n",
    "output_layer=[]\n",
    "for i in range(4):\n",
    "    output_layer=Conv2D(12+(2*i),3,activation='relu')((output_layer,input_layer)[i==0])\n",
    "    output_layer=MaxPool2D(2)(output_layer)\n",
    "\n",
    "output_layer=Flatten()(output_layer)\n",
    "\n",
    "for i in range(5):\n",
    "    output_layer=Dense(5)(output_layer)\n",
    "output_layer=Dense(1,activation='sigmoid')(output_layer)\n",
    "\n",
    "model=Model(input_layer,output_layer)\n",
    "model.compile(optimizer='adagrad',loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "model.fit_generator(fake_image(batch_size=15,seed=seed), validation_data=(data,labels),\n",
    "                    steps_per_epoch=30, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the trained model on unseen validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78ca6cf31b24645aad4fa45d9c9377c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='filename', options=('meteors_validation/0/1_0.jpg', 'meteors_validâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: show some samples of background / meteors\n",
    "import glob\n",
    "@interact\n",
    "def showimage(filename=sorted(glob.glob('meteors_validation/*/*jpg'))):\n",
    "    img=Image.open(filename)\n",
    "    img_arr=np.array(img)\n",
    "    print (\"Prediction: \", np.round(model.predict(img_arr.reshape([1,\n",
    "                                                                  img_arr.shape[0],\n",
    "                                                                  img_arr.shape[1],1]))))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only evaluated the model on a sample of 8 validation images. How well does our model perform on the validation and test datasets? In our real experimentation, we needed to take a more involved  approach to train an accurate model:\n",
    "\n",
    "<p><center><img src='assets/training.png'></center></p>\n",
    "\n",
    "Initially we generated a dataset with bright streaks and progressively made them fainter and fainter. \n",
    "\n",
    "<p><center><img src='assets/results.png'></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the convolutional layers of the trained model. This allows us to check our model to determine what pixels excite / activate our convolution filters and allows us to debug errors / difficult images (e.g. detecting very faint meteors on well-lit images of the night sky).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the layer\n",
    "def vis_layer(v, ix, iy, ch, cy, cx, p=0) :\n",
    "    v = np.reshape(v, (iy, ix, ch))\n",
    "    ix += 2\n",
    "    iy += 2\n",
    "    npad = ((1, 1), (1, 1), (0, 0))\n",
    "    v = np.pad(v, pad_width=npad, mode='constant', constant_values=p)\n",
    "    v = np.reshape(v, (iy, ix, cy, cx)) \n",
    "    v = np.transpose(v, (2,0,3,1)) #cy,iy,cx,ix\n",
    "    v = np.reshape(v, (cy*iy, cx*ix))\n",
    "    return v\n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output])\n",
    "    activations = get_activations([X_batch, 0])\n",
    "    return activations\n",
    "\n",
    "def plot_layer(feature_map, width, height, channels, rows, columns):\n",
    "    v  = vis_layer(feature_map, width, height, channels, rows, columns)\n",
    "    plt.figure(figsize = (16, 16))\n",
    "    plt.imshow(v, interpolation='nearest', cmap='gray')    \n",
    "    plt.axis('off');\n",
    "    \n",
    "def plot_activations(image):\n",
    "    img = Image.open(image)\n",
    "    img = np.array(img).reshape(1, 200, 200, 1)\n",
    "\n",
    "    conv_layers = [i for i, layer in enumerate(model.layers) if type(layer) == Conv2D]\n",
    "    print('Conv layers 1-%d' % (len(conv_layers)))\n",
    "    \n",
    "    for index, conv in enumerate(conv_layers):\n",
    "        feature_map = get_activations(model, conv, img)\n",
    "        image_shape = feature_map[0].shape[1]\n",
    "        n_filters = model.layers[conv].get_weights()[0].shape[3]\n",
    "        plot_layer(feature_map, image_shape, image_shape, n_filters, 2, int(n_filters / 2))\n",
    "\n",
    "files = sorted(glob.glob('meteors_validation/1/*jpg'))\n",
    "_ = interact(plot_activations, image=files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualisation gives us an insight to what the model is paying attention to. Ideally, we want the filters to activate for the streaks (i.e. pixels) and ignore / remove (despeckle) the stars from the image background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained an accurate model, we need to script a workflow that takes the original images and generate the detections. This process is depicted below:\n",
    "\n",
    "<p><center><img src='assets/detection.png'></center></p>\n",
    "\n",
    "The workflow scripts are quite involved and are not presented here. However, here is an example of the visualised detections for a given image.\n",
    "\n",
    "<p><center><img src='assets/test.jpg'></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew to download meteors.md5 file from us before he leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and uncompress the dataset. \n",
    "#Does nothing if the file already exists\n",
    "# Download(\"https://cloudstor.aarnet.edu.au/plus/s/EubVHMQ1lq7zOfD/download\",\"workshop.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew to wrap up about what was covered in the tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
